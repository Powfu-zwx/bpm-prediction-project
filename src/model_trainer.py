"""
æ¨¡å‹è®­ç»ƒæ¨¡å—
å®ç°å¤šç§æœºå™¨å­¦ä¹ æ¨¡å‹ç”¨äºBPMé¢„æµ‹
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from .config import *

class ModelTrainer:
    """BPMé¢„æµ‹æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self):
        self.models = {}
        self.best_model = None
        self.best_model_name = None
        self.model_results = {}
        
    def initialize_models(self):
        """åˆå§‹åŒ–å¤šä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹"""
        self.models = {
            'Linear Regression': LinearRegression(),
            'Ridge Regression': Ridge(alpha=1.0),
            'Lasso Regression': Lasso(alpha=1.0),
            'Random Forest': RandomForestRegressor(
                n_estimators=100,
                random_state=MODEL_PARAMS['random_state'],
                n_jobs=-1
            ),
            'Gradient Boosting': GradientBoostingRegressor(
                n_estimators=100,
                random_state=MODEL_PARAMS['random_state']
            ),
            'Support Vector Regression': SVR(kernel='rbf'),
            'K-Nearest Neighbors': KNeighborsRegressor(n_neighbors=5),
            'Multi-layer Perceptron': MLPRegressor(
                hidden_layer_sizes=(100, 50),
                random_state=MODEL_PARAMS['random_state'],
                max_iter=1000
            )
        }
        print(f"âœ… åˆå§‹åŒ–äº† {len(self.models)} ä¸ªæ¨¡å‹")
        
    def train_all_models(self, X_train, y_train, X_test, y_test):
        """è®­ç»ƒæ‰€æœ‰æ¨¡å‹å¹¶è¯„ä¼°æ€§èƒ½"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒæ‰€æœ‰æ¨¡å‹...")
        
        results = []
        
        for name, model in self.models.items():
            print(f"ğŸ“Š è®­ç»ƒ {name}...")
            
            try:
                # è®­ç»ƒæ¨¡å‹
                model.fit(X_train, y_train)
                
                # é¢„æµ‹
                y_pred_train = model.predict(X_train)
                y_pred_test = model.predict(X_test)
                
                # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
                train_mae = mean_absolute_error(y_train, y_pred_train)
                test_mae = mean_absolute_error(y_test, y_pred_test)
                train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
                test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
                train_r2 = r2_score(y_train, y_pred_train)
                test_r2 = r2_score(y_test, y_pred_test)
                
                # äº¤å‰éªŒè¯
                cv_scores = cross_val_score(model, X_train, y_train, cv=5, 
                                          scoring='neg_mean_absolute_error')
                cv_mae = -cv_scores.mean()
                
                result = {
                    'Model': name,
                    'Train MAE': train_mae,
                    'Test MAE': test_mae,
                    'Train RMSE': train_rmse,
                    'Test RMSE': test_rmse,
                    'Train RÂ²': train_r2,
                    'Test RÂ²': test_r2,
                    'CV MAE': cv_mae
                }
                
                results.append(result)
                self.model_results[name] = {
                    'model': model,
                    'metrics': result,
                    'predictions': {
                        'train': y_pred_train,
                        'test': y_pred_test
                    }
                }
                
                print(f"   âœ… Test MAE: {test_mae:.2f}, Test RÂ²: {test_r2:.3f}")
                
            except Exception as e:
                print(f"   âŒ {name} è®­ç»ƒå¤±è´¥: {str(e)}")
                
        # åˆ›å»ºç»“æœDataFrameå¹¶æ’åº
        self.results_df = pd.DataFrame(results)
        self.results_df = self.results_df.sort_values('Test MAE')
        
        # é€‰æ‹©æœ€ä½³æ¨¡å‹
        best_result = self.results_df.iloc[0]
        self.best_model_name = best_result['Model']
        self.best_model = self.model_results[self.best_model_name]['model']
        
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {self.best_model_name}")
        print(f"   Test MAE: {best_result['Test MAE']:.2f}")
        print(f"   Test RÂ²: {best_result['Test RÂ²']:.3f}")
        
        return self.results_df
    
    def hyperparameter_tuning(self, X_train, y_train, model_name=None):
        """è¶…å‚æ•°è°ƒä¼˜"""
        if model_name is None:
            model_name = self.best_model_name
            
        print(f"ğŸ”§ å¯¹ {model_name} è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜...")
        
        # å®šä¹‰è¶…å‚æ•°ç½‘æ ¼
        param_grids = {
            'Random Forest': {
                'n_estimators': [50, 100, 200],
                'max_depth': [None, 10, 20],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            },
            'Gradient Boosting': {
                'n_estimators': [50, 100, 200],
                'learning_rate': [0.01, 0.1, 0.2],
                'max_depth': [3, 5, 7],
                'subsample': [0.8, 0.9, 1.0]
            },
            'Support Vector Regression': {
                'C': [0.1, 1, 10],
                'gamma': ['scale', 'auto', 0.1, 1],
                'epsilon': [0.01, 0.1, 1]
            }
        }
        
        if model_name in param_grids:
            model = self.models[model_name]
            param_grid = param_grids[model_name]
            
            # ç½‘æ ¼æœç´¢
            grid_search = GridSearchCV(
                model, param_grid, 
                cv=5, 
                scoring='neg_mean_absolute_error',
                n_jobs=-1,
                verbose=1
            )
            
            grid_search.fit(X_train, y_train)
            
            # æ›´æ–°æœ€ä½³æ¨¡å‹
            self.best_model = grid_search.best_estimator_
            
            print(f"âœ… æœ€ä½³å‚æ•°: {grid_search.best_params_}")
            print(f"âœ… æœ€ä½³CVå¾—åˆ†: {-grid_search.best_score_:.2f}")
            
            return grid_search.best_params_
        else:
            print(f"âŒ {model_name} æ²¡æœ‰å®šä¹‰è¶…å‚æ•°ç½‘æ ¼")
            return None
    
    def get_feature_importance(self, feature_names):
        """è·å–ç‰¹å¾é‡è¦æ€§"""
        if hasattr(self.best_model, 'feature_importances_'):
            importance = self.best_model.feature_importances_
            feature_importance = pd.DataFrame({
                'feature': feature_names,
                'importance': importance
            }).sort_values('importance', ascending=False)
            
            return feature_importance
        else:
            print("âŒ å½“å‰æ¨¡å‹ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§åˆ†æ")
            return None
    
    def plot_model_comparison(self):
        """ç»˜åˆ¶æ¨¡å‹æ€§èƒ½æ¯”è¾ƒå›¾"""
        if self.results_df is not None:
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            
            # Test MAEæ¯”è¾ƒ
            axes[0, 0].barh(self.results_df['Model'], self.results_df['Test MAE'])
            axes[0, 0].set_title('ğŸ¯ Test MAE (è¶Šå°è¶Šå¥½)')
            axes[0, 0].set_xlabel('MAE')
            
            # Test RÂ²æ¯”è¾ƒ
            axes[0, 1].barh(self.results_df['Model'], self.results_df['Test RÂ²'])
            axes[0, 1].set_title('ğŸ“Š Test RÂ² (è¶Šå¤§è¶Šå¥½)')
            axes[0, 1].set_xlabel('RÂ²')
            
            # Train vs Test MAE
            x = np.arange(len(self.results_df))
            width = 0.35
            axes[1, 0].bar(x - width/2, self.results_df['Train MAE'], width, label='Train MAE')
            axes[1, 0].bar(x + width/2, self.results_df['Test MAE'], width, label='Test MAE')
            axes[1, 0].set_title('ğŸ”„ Train vs Test MAE')
            axes[1, 0].set_xlabel('Models')
            axes[1, 0].set_ylabel('MAE')
            axes[1, 0].set_xticks(x)
            axes[1, 0].set_xticklabels(self.results_df['Model'], rotation=45)
            axes[1, 0].legend()
            
            # CV MAE
            axes[1, 1].barh(self.results_df['Model'], self.results_df['CV MAE'])
            axes[1, 1].set_title('ğŸ”€ Cross-Validation MAE')
            axes[1, 1].set_xlabel('CV MAE')
            
            plt.tight_layout()
            plt.show()
    
    def save_best_model(self, filepath):
        """ä¿å­˜æœ€ä½³æ¨¡å‹"""
        if self.best_model is not None:
            joblib.dump({
                'model': self.best_model,
                'model_name': self.best_model_name,
                'results': self.results_df
            }, filepath)
            print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
        else:
            print("âŒ æ²¡æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹å¯ä¿å­˜")
    
    def load_model(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        model_data = joblib.load(filepath)
        self.best_model = model_data['model']
        self.best_model_name = model_data['model_name']
        if 'results' in model_data:
            self.results_df = model_data['results']
        print(f"âœ… æ¨¡å‹ {self.best_model_name} å·²ä» {filepath} åŠ è½½")
